{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x13c101890>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import requests\n",
    "device = 'mps'\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'The Project Gutenberg EBook of The Adventures of Sherlock Holmes by Sir Arthur Conan Doyle (#15 in our series by Sir Arthur Conan Doyle)  Copyright laws are changing all over the world. Be sure to check the copyright laws for your country before downloading or redistributing this or any other Project Gutenberg eBook.  This header should be the first thing seen when viewing this Project Gutenberg file.  Please do not remove it.  Do not change or edit the header without written permission.  Plea\n"
     ]
    }
   ],
   "source": [
    "res = requests.get('https://assets.datacamp.com/production/repositories/3937/datasets/213ca262bf6af12428d42842848464565f3d5504/sherlock.txt')\n",
    "dat = str((res.content))\n",
    "import re\n",
    "\n",
    "def replace_newlines(text):\n",
    "    pattern = r'\\\\n'\n",
    "    replaced_text = re.sub(pattern, ' ', text)\n",
    "    return replaced_text\n",
    "\n",
    "\n",
    "text = replace_newlines(dat)\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokz = AutoTokenizer.from_pretrained('openai-community/gpt2')\n",
    "\n",
    "tokenized_text = tokz.encode(text,truncation=True,return_overflowing_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(tokenized_text)) # first 90% will be train, rest val\n",
    "train_data = (tokenized_text[:n])\n",
    "val_data = tokenized_text[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split, batch_size=32):\n",
    "    # generate a batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    \n",
    "    # get the total number of sequences in the data\n",
    "    total_sequences = len(data)\n",
    "    \n",
    "    # generate random indices to select batches\n",
    "    ix = torch.randint(total_sequences, (batch_size,))\n",
    "    # select sequences from the data using the random indices\n",
    "    \n",
    "    x = torch.stack([torch.tensor(data[i]) for i in ix])\n",
    "    y = torch.stack([torch.cat((seq[1:], seq[0].unsqueeze(0))) for seq in x])\n",
    "    \n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "x,y = get_batch('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1024, 384])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "eval_iters = 200\n",
    "learning_rate = 3e-4\n",
    "device = 'mps'\n",
    "n_embed = 384 # has to be divisible(without rem) by n_head, given head_size definition further below\n",
    "n_head = 6\n",
    "n_layer = 7\n",
    "dropout = 0.3\n",
    "vocab_size = len(tokz.get_vocab())\n",
    "block_size = 1024\n",
    "\n",
    "# single head of self attention\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        # in Pytorch convention a variable that's not a parameter of the model is called a buffer\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "    def forward(self, x): # 32 x 1024 x 384 \n",
    "        B,T,C = x.shape\n",
    "        # emit keys and queries for x\n",
    "        k = self.key(x)  # (B, T, hs) 32 x 1024 x 64\n",
    "        q = self.query(x) # (B, T, hs) 32 x 1024 x 64\n",
    "\n",
    "        # compute attention\n",
    "        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T) 32 x 1024 x 1024\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T) 32 x 1024 x 1024\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T) 32 x 1024 x 1024\n",
    "        wei = self.dropout(wei) \n",
    "        v = self.value(x) # (B, T, hs) 32 x 1024 x 64\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs) # 32 x 1024 x 64\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_heads = 6, head_size = 64):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embed, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.proj(out) # outcome of the linear layer to project back into the residual pathway\n",
    "        out = self.dropout(out) # final dropout\n",
    "        return out\n",
    "\n",
    "x = torch.rand(32,1024,384)\n",
    "mod = MultiHeadAttention()\n",
    "mod(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self attention \"\"\"\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.beta = nn.Parameter(torch.tensor(0.0))  # Initialize beta as a learnable parameter\n",
    "\n",
    "    def forward(self, x, a_mem_prev=None, z_prev=None):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)   # (B, T, hs)\n",
    "        q = self.query(x) # (B, T, hs)\n",
    "        v = self.value(x) # (B, T, hs)\n",
    "        # Compute attention weights (A_dot)\n",
    "\n",
    "        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5  # (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        # Compute M_s and Z_s\n",
    "        m_s = torch.nn.functional.elu(k.transpose(-2, -1)) + 1  # (B, hs, T)\n",
    "        m_s = m_s @ v  # (B, hs, hs)\n",
    "        print(m_s.shape)\n",
    "        z_s = torch.sum(torch.nn.functional.elu(k) + 1, dim=-2)  # (B, hs)\n",
    "        # Compute A_mem\n",
    "        if a_mem_prev is None:\n",
    "            a_mem = None\n",
    "        else:\n",
    "            phi_q = torch.nn.functional.elu(q) + 1\n",
    "            print(f'phi_q:{phi_q.shape}')\n",
    "            print(a_mem_prev.shape)\n",
    "            a_mem = (torch.nn.functional.elu(q) + 1) * (a_mem_prev / (torch.nn.functional.elu(q) + 1) * z_prev.unsqueeze(-1))\n",
    "\n",
    "        # Compute final attention (A)\n",
    "        if a_mem is None:\n",
    "            a = wei  # A_dot\n",
    "        else:\n",
    "            a = torch.sigmoid(self.beta) * a_mem + (1 - torch.sigmoid(self.beta)) * wei\n",
    "\n",
    "        out = a @ v  # (B, T, hs)\n",
    "        return out, m_s, z_s\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads=6, head_size=64):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embed, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        a_mem_prev = None\n",
    "        z_prev = None\n",
    "        head_outputs = []\n",
    "\n",
    "        for head in self.heads:\n",
    "            out, m_s, z_s = head(x, a_mem_prev, z_prev)\n",
    "            head_outputs.append(out)\n",
    "            a_mem_prev = m_s\n",
    "            z_prev = z_s\n",
    "\n",
    "        out = torch.cat(head_outputs, dim=-1)\n",
    "        out = self.proj(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "x = torch.rand(32,1024,384)\n",
    "mod = MultiHeadAttention()\n",
    "mod(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \" simple linear layer followed by non linearity \"\n",
    "    \n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4 * n_embed), # as mentioned in the paper\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embed, n_embed), # projection layer : the final projection back into the residual pathway\n",
    "            nn.Dropout(dropout), # dropout before final projection\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" a transformer block : communication then computation \"\"\"\n",
    "    \n",
    "    def __init__(self, n_embed, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embed)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class LanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embed, n_head=n_head) for _ in range(n_layer)]) \n",
    "        self.ln_f = nn.LayerNorm(n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        token_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
    "        x = token_emb + pos_emb # (B, T, C)\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)# (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block size token\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = LanguageModel()\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "m = model.to(device)\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    # interesting that we're not printing loss every iter\n",
    "    # instead we're estimating the non noisy loss every eval_intervar\n",
    "    # only for printing purposes\n",
    "    if (iter % eval_interval == 0) or (iter == max_iters-1):\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "output = tokz.decode(m.generate(context, max_new_tokens=500)[0].tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
