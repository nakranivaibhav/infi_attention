{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import requests\n",
    "import math\n",
    "\n",
    "device = torch.device('mps')\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get('https://assets.datacamp.com/production/repositories/3937/datasets/213ca262bf6af12428d42842848464565f3d5504/sherlock.txt')\n",
    "dat = str((res.content))\n",
    "import re\n",
    "\n",
    "def replace_newlines(text):\n",
    "    pattern = r'\\\\n'\n",
    "    replaced_text = re.sub(pattern, ' ', text)\n",
    "    return replaced_text\n",
    "\n",
    "\n",
    "text = replace_newlines(dat)\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokz = AutoTokenizer.from_pretrained('openai-community/gpt2')\n",
    "\n",
    "tokenized_text = tokz.encode(text,truncation=True,return_overflowing_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(tokenized_text)) # first 90% will be train, rest val\n",
    "train_data = (tokenized_text[:n])\n",
    "val_data = tokenized_text[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split, batch_size=32):\n",
    "    # generate a batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    \n",
    "    # get the total number of sequences in the data\n",
    "    total_sequences = len(data)\n",
    "    \n",
    "    # generate random indices to select batches\n",
    "    ix = torch.randint(total_sequences, (batch_size,))\n",
    "    # select sequences from the data using the random indices\n",
    "    \n",
    "    x = torch.stack([torch.tensor(data[i]) for i in ix])\n",
    "    y = torch.stack([torch.cat((seq[1:], seq[0].unsqueeze(0))) for seq in x])\n",
    "    \n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "x,y = get_batch('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "eval_iters = 200\n",
    "learning_rate = 3e-4\n",
    "device = 'mps'\n",
    "n_embed = 384 # has to be divisible(without rem) by n_head, given head_size definition further below\n",
    "n_head = 6\n",
    "n_layer = 7\n",
    "dropout = 0.3\n",
    "vocab_size = len(tokz.get_vocab())\n",
    "block_size = 1024\n",
    "head_size = n_embed // n_head\n",
    "\n",
    "# single head of self attention\n",
    "class InfiniAttention(nn.Module):\n",
    "    \"\"\" one head of self attention \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.n_embed = n_embed\n",
    "        self.num_heads = n_head\n",
    "        self.head_dim = self.n_embed // self.num_heads\n",
    "        self.attention_dropout = nn.Dropout(p = 0.1)\n",
    "        \n",
    "        self.q_proj = nn.Linear(self.n_embed,self.num_heads * self.head_dim,bias = True)\n",
    "        self.k_proj = nn.Linear(self.n_embed,self.num_heads * self.head_dim,bias = True)\n",
    "        self.v_proj = nn.Linear(self.n_embed,self.num_heads * self.head_dim,bias = True)\n",
    "        self.o_proj = nn.Linear(self.num_heads * self.head_dim,self.n_embed,bias = False)\n",
    "\n",
    "        # In Pytorch convention a variable that's not a parameter of the model is called a buffer\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.attention_drop_p = 0.1\n",
    "        \n",
    "        self.beta = nn.Parameter(torch.rand(1))\n",
    "        self.register_buffer(\"M\", torch.zeros(self.num_heads, self.head_dim, self.head_dim))\n",
    "        self.register_buffer(\"z\", torch.zeros(self.num_heads, self.head_dim))\n",
    "\n",
    "    def forward(self, x:torch.tensor): # 32 x 1024 x 384 \n",
    "        bs,q_len,_ = x.size()\n",
    "        print(bs),print(q_len)\n",
    "        q_states = self.q_proj(x) # 32 x 1024 x 384\n",
    "        k_states = self.k_proj(x) # 32 x 1024 x 384\n",
    "        v_states = self.v_proj(x) # 32 x 1024 x 384\n",
    "\n",
    "        q_states = q_states.view(bs, q_len, self.num_heads, self.head_dim).transpose(1,2) # 32 x 6 x 1024 x 64\n",
    "        k_states = k_states.view(bs, q_len, self.num_heads, self.head_dim).transpose(1,2) # 32 x 6 x 1024 x 64\n",
    "        v_states = v_states.view(bs, q_len, self.num_heads, self.head_dim).transpose(1,2) # 32 x 6 x 1024 x 64\n",
    "        \n",
    "        kv_seq_len = k_states.shape[-2] # 1024\n",
    "        attention_mask = torch.tril(torch.ones(q_len, kv_seq_len)).unsqueeze(0).repeat(bs, 1, 1, 1)\n",
    "        attention_mask = attention_mask.to(x.device)\n",
    "    \n",
    "        if attention_mask.size() != (bs, 1, q_len, kv_seq_len):\n",
    "            raise ValueError(\n",
    "                f\"Attention mask should be of size {(bs, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n",
    "            )\n",
    "        A_mem = self._retrieve(q_states, self.M, self.z)\n",
    "        self.M, self.z = self._update(k_states, v_states, self.M, self.z) \n",
    "\n",
    "        A_dot = F.scaled_dot_product_attention(\n",
    "            q_states,\n",
    "            k_states,\n",
    "            v_states,\n",
    "            attn_mask = attention_mask,\n",
    "            dropout_p = self.attention_drop_p\n",
    "        )\n",
    "\n",
    "        final_attention = self._attn_mul(A_mem = A_mem,A_dot = A_dot)\n",
    "        \n",
    "        final_attention = final_attention.transpose(1, 2).contiguous()\n",
    "        final_attention = final_attention.view(bs, q_len, self.n_embed)\n",
    "        out = self.o_proj(final_attention)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def _retrieve(self, Q, M, z):\n",
    "        # Retrieve from compressive memory eq.3\n",
    "        M_s_1 = torch.matmul(F.elu(Q) + 1, M)\n",
    "        z_s_1 = torch.matmul(F.elu(Q) + 1, z.unsqueeze(-1)) + 1e-8\n",
    "        A_mem = M_s_1 / z_s_1\n",
    "        return A_mem\n",
    "    \n",
    "    def _update(self, K, V, M, z, use_delta = False):\n",
    "        eps = 1e-8\n",
    "        if use_delta:\n",
    "            v_retrieved = torch.matmul(F.elu(K) + 1, M) / torch.matmul(F.elu(K) + 1, z.unsqueeze(-1) + eps)\n",
    "            updated_m = M + torch.matmul(F.elu(K).transpose(-1,-2) + 1, V - v_retrieved)\n",
    "        else:\n",
    "            updated_m = M  + torch.matmul(F.elu(K).transpose(-1,-2) + 1, V)\n",
    "\n",
    "        updated_z = z + (F.elu(K) + 1).sum(dim = -2)\n",
    "        M = updated_m\n",
    "        z = updated_z\n",
    "\n",
    "        return M, z\n",
    "    \n",
    "    def _attn_mul(self, A_dot, A_mem):\n",
    "        beta = torch.sigmoid(self.beta)\n",
    "        A = beta * A_mem + (1 - beta) * A_dot\n",
    "        return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \" simple linear layer followed by non linearity \"\n",
    "    \n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4 * n_embed), # as mentioned in the paper\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embed, n_embed), # projection layer : the final projection back into the residual pathway\n",
    "            nn.Dropout(dropout), # dropout before final projection\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" a transformer block : communication then computation \"\"\"\n",
    "    \n",
    "    def __init__(self, n_embed, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_head\n",
    "        self.sa = InfiniAttention()\n",
    "        self.ffwd = FeedForward(n_embed)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class LanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embed, n_head=n_head) for _ in range(n_layer)]) \n",
    "        self.ln_f = nn.LayerNorm(n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        token_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
    "        x = token_emb + pos_emb # (B, T, C)\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)# (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block size token\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = LanguageModel()\n",
    "model.to(device)\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            X.to(device),Y.to(device)\n",
    "            _ , loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "m = model.to(device)\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    # interesting that we're not printing loss every iter\n",
    "    # instead we're estimating the non noisy loss every eval_intervar\n",
    "    # only for printing purposes\n",
    "    if (iter % eval_interval == 0) or (iter == max_iters-1):\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    xb.to(device),yb.to(device)\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# # generate from the model\n",
    "# context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "# output = tokz.decode(m.generate(context, max_new_tokens=500)[0].tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
